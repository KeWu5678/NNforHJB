{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeebd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to Python path so model.py can find ssn and net modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from src.model import model\n",
    "from src.model_outerweights import model_outerweights\n",
    "from src.greedy_insertion import insertion\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dfa9fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 11:48:48.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mLoaded data with shape: (1800,), dtype: [('x', '<f8', (2,)), ('dv', '<f8', (2,)), ('v', '<f8')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "path = '../data_result/raw_data/VDP_beta_0.1_grid_combined.npy'# Initialize the weights\n",
    "data = np.load(path)\n",
    "logger.info(f\"Loaded data with shape: {data.shape}, dtype: {data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4099fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameter\n",
    "power = 2\n",
    "gamma = 5.0\n",
    "M = 100 # number greedy insertion selected\n",
    "alpha = 1e-5\n",
    "regularization = (gamma, alpha) \n",
    "num_iterations = 20\n",
    "loss_weights = (1.0, 1.0)\n",
    "pruning_threshold = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4273cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_configure_logger\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mVDPModel initialized\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36m_configure_logger\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mVDPModel (outer weights) initialized\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model \n",
    "model_1 = model(data, torch.relu, power, regularization, optimizer='Adam', loss_weights = loss_weights)\n",
    "model_2 = model_outerweights(data, torch.relu, power, regularization, optimizer='SSN', loss_weights = loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e2c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the initializing weights and bias\n",
    "init_weights = np.random.randn(M, 2) * 0.1\n",
    "init_bias = np.random.randn(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256a18d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1mStarting network training session\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mTraining set: 1620 samples, Validation set: 180 samples\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mData ranges - x: [-3.00, 3.00], v: [0.00, 10.96], dv: [-13.19, 13.19]\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_create_network\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mCreating network with 100 neurons\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mUsing Adam optimizer with lr=0.0001\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mTraining model, saving to /Users/ruizhechao/Documents/NNforHJB/train_history\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mTraining hyperparameters: iterations=5000, batch_size=1620, display_every=1000\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m266\u001b[0m - \u001b[1mLoss weights: value=1.0, gradient=1.0\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mEpoch 0: Train Loss = 31.794406, Val Loss = 33.011372\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mEpoch 1000: Train Loss = 13.874980, Val Loss = 11.649585\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mEpoch 2000: Train Loss = 5.231260, Val Loss = 4.948171\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mEpoch 3000: Train Loss = 3.751133, Val Loss = 4.094290\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mEpoch 4000: Train Loss = 3.447435, Val Loss = 4.011829\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m324\u001b[0m - \u001b[1mFinal model saved to /Users/ruizhechao/Documents/NNforHJB/train_history/model_final.pt\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mTraining completed successfully\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitialization done\u001b[0m\n",
      "\u001b[32m2025-08-04 11:48:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitial weights shape: (100, 2), bias shape: (100,)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_result, weight_raw, bias_raw, outerweight_raw = model_1.train(\n",
    "    iterations=5000,\n",
    "    display_every=1000,\n",
    "    inner_weights=init_weights, inner_bias=init_bias,\n",
    ")\n",
    "logger.info(\"Initialization done\"); logger.info(f\"Initial weights shape: {weight_raw.shape}, bias shape: {bias_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b80147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mIteration 0 - current weights shape: (100, 2), current bias shape: (100,)\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1mStarting network training session (outer weights only)\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mTraining set: 1620 samples, Validation set: 180 samples\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m129\u001b[0m - \u001b[1mData ranges - x: [-3.00, 3.00], v: [0.00, 10.96], dv: [-13.19, 13.19]\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mUsing SSN optimizer with alpha=1e-05, gamma=5.0\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m261\u001b[0m - \u001b[1mTraining model, saving to /Users/ruizhechao/Documents/NNforHJB/train_history\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mTraining hyperparameters: iterations=20000, batch_size=1620, display_every=1000\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mLoss weights: value=1.0, gradient=1.0\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model_outerweights\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mEpoch 0: Train Loss = 7.503822, Val Loss = 11.705756\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:37\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:38\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:39\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:40\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:41\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m304\u001b[0m - \u001b[33m\u001b[1mTheta reached 7.0e-16. Breaking line search loop.\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m313\u001b[0m - \u001b[33m\u001b[1munew contains Inf/NaN – increasing damping (theta *= 4) and retrying\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m339\u001b[0m - \u001b[33m\u001b[1mLine search failed after 1 iterations\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m340\u001b[0m - \u001b[33m\u001b[1mFinal lossective values: loss=inf, loss_new=NaN\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m341\u001b[0m - \u001b[33m\u001b[1mFinal theta: nan\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m313\u001b[0m - \u001b[33m\u001b[1munew contains Inf/NaN – increasing damping (theta *= 4) and retrying\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m339\u001b[0m - \u001b[33m\u001b[1mLine search failed after 1 iterations\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m340\u001b[0m - \u001b[33m\u001b[1mFinal lossective values: loss=inf, loss_new=NaN\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m341\u001b[0m - \u001b[33m\u001b[1mFinal theta: nan\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m313\u001b[0m - \u001b[33m\u001b[1munew contains Inf/NaN – increasing damping (theta *= 4) and retrying\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m339\u001b[0m - \u001b[33m\u001b[1mLine search failed after 1 iterations\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m340\u001b[0m - \u001b[33m\u001b[1mFinal lossective values: loss=inf, loss_new=NaN\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m341\u001b[0m - \u001b[33m\u001b[1mFinal theta: nan\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m313\u001b[0m - \u001b[33m\u001b[1munew contains Inf/NaN – increasing damping (theta *= 4) and retrying\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m339\u001b[0m - \u001b[33m\u001b[1mLine search failed after 1 iterations\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m340\u001b[0m - \u001b[33m\u001b[1mFinal lossective values: loss=inf, loss_new=NaN\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m341\u001b[0m - \u001b[33m\u001b[1mFinal theta: nan\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m313\u001b[0m - \u001b[33m\u001b[1munew contains Inf/NaN – increasing damping (theta *= 4) and retrying\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m339\u001b[0m - \u001b[33m\u001b[1mLine search failed after 1 iterations\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m340\u001b[0m - \u001b[33m\u001b[1mFinal lossective values: loss=inf, loss_new=NaN\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m341\u001b[0m - \u001b[33m\u001b[1mFinal theta: nan\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m346\u001b[0m - \u001b[31m\u001b[1mSSN optimizer has failed 5 consecutive times. This may indicate:\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m347\u001b[0m - \u001b[31m\u001b[1m  1. The problem is not suitable for SSN optimization\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m348\u001b[0m - \u001b[31m\u001b[1m  2. The regularization parameters (alpha, gamma) are inappropriate\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m349\u001b[0m - \u001b[31m\u001b[1m  3. The gradient computation is incorrect\u001b[0m\n",
      "\u001b[32m2025-08-04 11:49:42\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mssn\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m350\u001b[0m - \u001b[31m\u001b[1m  Consider switching to Adam optimizer or adjusting parameters.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SSN optimizer failed repeatedly - stopping to prevent infinite loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):  \n\u001b[1;32m      3\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - current weights shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_raw\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, current bias shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias_raw\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 4\u001b[0m     model, weight, bias, outerweights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouter_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mouterweight_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Convert to flat array and count elements with absolute value less than threshold\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     outerweights_raw \u001b[38;5;241m=\u001b[39m outerweight_raw\u001b[38;5;241m.\u001b[39mflatten() \n",
      "File \u001b[0;32m~/Documents/NNforHJB/src/model_outerweights.py:291\u001b[0m, in \u001b[0;36mmodel_outerweights.train\u001b[0;34m(self, inner_weights, inner_bias, outer_weights, iterations, batch_size, display_every)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Compute loss and backprop\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, SSN):\n\u001b[0;32m--> 291\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m     total_loss, value_loss, grad_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[1;32m    294\u001b[0m         train_x_tensor, train_v_tensor, train_dv_tensor\n\u001b[1;32m    295\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/NNforHJB/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NNforHJB/src/ssn.py:351\u001b[0m, in \u001b[0;36mSSN.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    349\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  3. The gradient computation is incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    350\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Consider switching to Adam optimizer or adjusting parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSN optimizer failed repeatedly - stopping to prevent infinite loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# Restore original parameters\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_parameters(params)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SSN optimizer failed repeatedly - stopping to prevent infinite loop"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for i in range(num_iterations - 1):  \n",
    "    logger.info(f\"Iteration {i} - current weights shape: {weight_raw.shape}, current bias shape: {bias_raw.shape}\") \n",
    "    model, weight, bias, outerweights = model_2.train(inner_weights = weight_raw, inner_bias = bias_raw, outer_weights = outerweight_raw)\n",
    "            \n",
    "    # Convert to flat array and count elements with absolute value less than threshold\n",
    "    outerweights_raw = outerweight_raw.flatten() \n",
    "    outerweights = outerweights.flatten()\n",
    "    small_weights_count = np.sum(np.abs(outerweights_raw) < pruning_threshold)\n",
    "    small_weights_filtered_count = np.sum(np.abs(outerweights) < pruning_threshold)\n",
    "    \n",
    "    logger.info(f\"1st model weights shape: {np.shape(outerweight_raw)}, 2nd model weights shape: {np.shape(outerweights)}, Pruned neurons in 2nd model with abs value < {pruning_threshold}: {small_weights_count}\")\n",
    "    \n",
    "    # insert M neurons\n",
    "    weight_temp, bias_temp = insertion(data, model_result, M, alpha)\n",
    "    # weight_temp and bias_temp are already numpy arrays from insertion()\n",
    "    weights = np.concatenate((weight, weight_temp), axis=0)\n",
    "    biases = np.concatenate((bias, bias_temp), axis=0)\n",
    "    logger.info(f\"Iteration {i} - inserted weights shape: {weight_temp.shape}, inserted bias shape: {bias_temp.shape}\")\n",
    "    \n",
    "    # train 1st model with adam than accelarate with 2nd model with ssn\n",
    "    model_result, weight_raw, bias_raw, outerweight_raw = model_1.train(inner_weights=weights, inner_bias=biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b642d",
   "metadata": {},
   "source": [
    "The problem is that the line search always fails. Observe the variable loss_new,   after damping the paramter the loss is not decreased and eventually an error comes out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d338fb",
   "metadata": {},
   "source": [
    "\n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:259 - Solution (dq) norm: 2.743474e-11**  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:277 - Initial theta0: 6.042138e+10, step norm: 2.743474e-11\n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 0: theta=6.04e+10, **loss_new=4.355797e+00**   \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 1: theta=1.51e+10, **loss_new=4.355887e+00**    \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 2: theta=3.78e+09, **loss_new=4.355887e+00**  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 3: theta=9.44e+08, **loss_new=4.355887e+00**  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 4: theta=2.36e+08, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 5: theta=5.90e+07, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 6: theta=1.48e+07, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 7: theta=3.69e+06, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 8: theta=9.22e+05, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 9: theta=2.30e+05, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 20: theta=5.50e-02, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:308 - Damping step 40: theta=5.00e-14, loss_new=4.355887e+00  \n",
    "2025-08-04 10:18:08 | WARNING  | ssn:step:304 - Theta reached 7.8e-16. Breaking line search loop.  \n",
    "2025-08-04 10:18:08 | INFO     | src.model_outerweights:train:308 - Epoch 0: Train Loss = 2.915392, Val Loss = 6.527839  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:step:210 - Initial loss: 4.355797e+00, penalty: 9.064044e-05  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:_transform_param2q:177 - Gradient computed successfully, norm: 2.274264e+01  \n",
    "2025-08-04 10:18:08 | DEBUG    | ssn:_Hessian:152 - Using correct Gauss-Newton Hessian: S^T*S*DPc, S shape: torch.Size([1620, 100])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
