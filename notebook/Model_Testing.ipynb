{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the src directory to Python path so model.py can find ssn and net modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c2f23",
   "metadata": {},
   "source": [
    "Load the data that is generated from the open-loop optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ff901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 02:04:16.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mLoaded data with shape: (900,), dtype: [('x', '<f8', (2,)), ('dv', '<f8', (2,)), ('v', '<f8')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "path = '../data_result/raw_data/VDP_beta_0.1_grid_30x30.npy'# Initialize the weights\n",
    "data = np.load(path)\n",
    "logger.info(f\"Loaded data with shape: {data.shape}, dtype: {data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755bd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78da1f3f",
   "metadata": {},
   "source": [
    "## SSN(line-search) method for outer weights ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3914c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameter\n",
    "power = 2.1\n",
    "M = 50 # number greedy insertion selected\n",
    "num_iterations = 10\n",
    "loss_weights = (1.0, 0.0)\n",
    "pruning_threshold = 1e-15\n",
    "\n",
    "gamma = 5.0\n",
    "alpha = 1e-5\n",
    "lr_adam = 1e-5\n",
    "regularization = (gamma, alpha) \n",
    "th = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b289993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 02:27:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_configure_logger\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mModel initialized\u001b[0m\n",
      "\u001b[32m2025-09-22 02:27:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mTraining set: 810 samples, Validation set: 90 samples\u001b[0m\n",
      "\u001b[32m2025-09-22 02:27:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mData ranges - x: [-3.00, 3.00], v: [0.00, 10.96], dv: [-13.19, 13.13]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.model import model\n",
    "test = model(activation=torch.relu, power=power, regularization=regularization, optimizer='SSN_TR', loss_weights=loss_weights, th=th, train_outerweights=True)\n",
    "\n",
    "# prepare the data\n",
    "data_train, data_valid = test._prepare_data(data)\n",
    "\n",
    "from src.greedy_insertion import _sample_uniform_sphere_points\n",
    "W_hidden, b_hidden = _sample_uniform_sphere_points(M)\n",
    "# test.train(data_train, data_valid, inner_weights=W_hidden, inner_bias=b_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f27bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 02:27:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m227\u001b[0m - \u001b[1mUsing SSN_TR optimizer with alpha=1e-05, gamma=5.0, th=0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.ssn import SSN\n",
    "from src.ssn_tr import SSN_TR\n",
    "\n",
    "# Build net once if needed\n",
    "if test.net is None:\n",
    "    test._create_network(inner_weights=W_hidden, inner_bias=b_hidden)\n",
    "\n",
    "# Build optimizer once if needed\n",
    "if test.optimizer is None:\n",
    "    test._setup_optimizer()\n",
    "\n",
    "# Define closure for the current data tensors\n",
    "train_x_tensor, train_v_tensor, train_dv_tensor = data_train\n",
    "def closure():\n",
    "    if isinstance(test.optimizer, (SSN, SSN_TR)):\n",
    "        with torch.no_grad():\n",
    "            _, hidden_activations = test.net.forward_with_hidden(train_x_tensor.detach())\n",
    "        test.optimizer.hidden_activations = hidden_activations.detach()\n",
    "    total_loss, _, _ = test._compute_loss(train_x_tensor, train_v_tensor, train_dv_tensor)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6f11963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Gq||: 3.1245061830789997e-15 ||lhs - rhs||: 0.0\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _ddphi\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "q = test.optimizer._transform_param2q(params, loss)\n",
    "Gq = test.optimizer._Gradient(q, params, loss)\n",
    "\n",
    "grads = torch.autograd.grad(loss, test.optimizer.param_groups[0][\"params\"], create_graph=True, retain_graph=True)\n",
    "grad_flat = torch.cat([g.view(-1) for g in grads])\n",
    "D_nonconvex = torch.sign(params) * (_ddphi(torch.abs(params), test.th, test.gamma) - 1)\n",
    "\n",
    "lhs = Gq\n",
    "rhs = test.optimizer.c * (q - params) + test.alpha * D_nonconvex + grad_flat\n",
    "print(\"||Gq||:\", float(torch.norm(lhs)), \"||lhs - rhs||:\", float(torch.norm(lhs - rhs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0183764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at t grid from params->prox(q):\n",
      "t=0.0 loss=14.882359343530393\n",
      "t=0.1 loss=2387.4469079444934\n",
      "t=0.2 loss=10186.331179815632\n",
      "t=0.3 loss=23411.536390744655\n",
      "t=0.4 loss=42063.058934943285\n",
      "t=0.5 loss=66140.90045274328\n",
      "t=0.6 loss=95645.07047727126\n",
      "t=0.7 loss=130575.54022679082\n",
      "t=0.8 loss=170932.35134168164\n",
      "t=0.9 loss=216715.4557128873\n",
      "t=1.0 loss=267924.9079181408\n",
      "Δloss at t=1: tensor(267910.0256, dtype=torch.float64, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _compute_prox\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss0 = closure()\n",
    "\n",
    "mu = test.alpha / test.optimizer.c\n",
    "q = test.optimizer._transform_param2q(params, loss0)\n",
    "unew = _compute_prox(q, mu)\n",
    "\n",
    "vals = []\n",
    "ts = torch.linspace(0, 1, steps=11)\n",
    "for t in ts:\n",
    "    u_t = params*(1 - t) + unew*t\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    vals.append(float(closure()))\n",
    "    test.optimizer._update_parameters(backup)\n",
    "\n",
    "print(\"loss at t grid from params->prox(q):\")\n",
    "for t, v in zip(ts, vals):\n",
    "    print(f\"t={float(t):.1f} loss={v}\")\n",
    "print(\"Δloss at t=1:\", vals[-1] - loss0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2676e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpcg: flag radius pred -27.3105670419037 relres 0.1519114752983517 iters 3 ||dq|| 1.0\n",
      "t=1.00000 loss=13.68151188778112 Δ=-1.2008474557492725\n",
      "t=0.50000 loss=1.2271700255679998 Δ=-13.655189317962392\n",
      "t=0.25000 loss=4.790326326920983 Δ=-10.09203301660941\n",
      "t=0.12500 loss=9.019587950729846 Δ=-5.862771392800546\n",
      "t=0.06250 loss=11.746075485041867 Δ=-3.1362838584885253\n",
      "t=0.03125 loss=13.262394543528318 Δ=-1.6199648000020748\n",
      "best t: 0.5 best loss: 1.2271700255679998 Δbest: -13.655189317962392\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _compute_dprox, _compute_prox\n",
    "\n",
    "# Freeze current state\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "# Use q = params to avoid algebraic cancellation\n",
    "q = params.clone().detach()\n",
    "Gq = test.optimizer._Gradient(q, params, loss)          # now ≈ alpha*D_nonconvex + grad_flat\n",
    "DG = test.optimizer._Hessian(q, params, loss)\n",
    "mu = test.alpha / test.optimizer.c\n",
    "DP = _compute_dprox(q, mu)\n",
    "\n",
    "# One MPCG step\n",
    "from src.mpcg import mpcg\n",
    "I_active = (torch.diagonal(DP) != 0)\n",
    "kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "sigma = test.optimizer.sigma if isinstance(test.optimizer, (SSN_TR,)) else 0.0\n",
    "\n",
    "dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "print(\"mpcg: flag\", flag, \"pred\", pred, \"relres\", relres, \"iters\", iters, \"||dq||\", float(torch.norm(dq)))\n",
    "\n",
    "# Try a backtracking grid along dq: q_new = q + t*dq, u_new = prox(q_new)\n",
    "ts = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125]\n",
    "best = (None, float('inf'))\n",
    "for t in ts:\n",
    "    q_t = q + t * dq\n",
    "    u_t = _compute_prox(q_t, mu)\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    loss_t = float(closure())\n",
    "    test.optimizer._update_parameters(backup)\n",
    "    print(f\"t={t:.5f} loss={loss_t} Δ={loss_t - float(loss)}\")\n",
    "    if loss_t < best[1]:\n",
    "        best = (t, loss_t)\n",
    "print(\"best t:\", best[0], \"best loss:\", best[1], \"Δbest:\", best[1] - float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d31c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma: 1.0\n",
      "active (DP>0): 50\n"
     ]
    }
   ],
   "source": [
    "print(\"sigma:\", float(getattr(test.optimizer, \"sigma\", 0.0)))\n",
    "DP = __import__(\"src.utils\", fromlist=[\"_compute_dprox\"])._compute_dprox(\n",
    "    torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]]),\n",
    "    test.alpha / test.optimizer.c\n",
    ")\n",
    "print(\"active (DP>0):\", int((torch.diagonal(DP) > 0).sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb43883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpcg: flag radius pred -27.3105670419037 relres 0.1519114752983517 iters 3 ||dq|| 1.0\n",
      "t=1.00000 loss=13.68151188778112 Δ=-1.2008474557492725\n",
      "t=0.50000 loss=1.2271700255679998 Δ=-13.655189317962392\n",
      "t=0.25000 loss=4.790326326920983 Δ=-10.09203301660941\n",
      "t=0.12500 loss=9.019587950729846 Δ=-5.862771392800546\n",
      "t=0.06250 loss=11.746075485041867 Δ=-3.1362838584885253\n",
      "t=0.03125 loss=13.262394543528318 Δ=-1.6199648000020748\n",
      "best t: 0.5 best loss: 1.2271700255679998 Δbest: -13.655189317962392\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _compute_dprox, _compute_prox, _ddphi\n",
    "from src.mpcg import mpcg\n",
    "from src.ssn import SSN\n",
    "from src.ssn_tr import SSN_TR\n",
    "\n",
    "train_x_tensor, train_v_tensor, train_dv_tensor = data_train\n",
    "\n",
    "def closure():\n",
    "    if isinstance(test.optimizer, (SSN, SSN_TR)):\n",
    "        with torch.no_grad():\n",
    "            _, S = test.net.forward_with_hidden(train_x_tensor.detach())\n",
    "        test.optimizer.hidden_activations = S.detach()\n",
    "    total_loss, _, _ = test._compute_loss(train_x_tensor, train_v_tensor, train_dv_tensor)\n",
    "    return total_loss\n",
    "\n",
    "# Freeze state\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "# q = params\n",
    "q = params.clone().detach()\n",
    "Gq = test.optimizer._Gradient(q, params, loss)\n",
    "DG = test.optimizer._Hessian(q, params, loss)\n",
    "mu = test.alpha / test.optimizer.c\n",
    "DP = _compute_dprox(q, mu)\n",
    "\n",
    "I_active = (torch.diagonal(DP) != 0)\n",
    "kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "\n",
    "dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "print(\"mpcg: flag\", flag, \"pred\", pred, \"relres\", relres, \"iters\", iters, \"||dq||\", float(torch.norm(dq)))\n",
    "\n",
    "# Backtracking along dq\n",
    "ts = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125]\n",
    "best = (None, float('inf'))\n",
    "for t in ts:\n",
    "    q_t = q + t * dq\n",
    "    u_t = _compute_prox(q_t, mu)\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    loss_t = float(closure())\n",
    "    test.optimizer._update_parameters(backup)\n",
    "    print(f\"t={t:.5f} loss={loss_t} Δ={loss_t - float(loss)}\")\n",
    "    if loss_t < best[1]:\n",
    "        best = (t, loss_t)\n",
    "print(\"best t:\", best[0], \"best loss:\", best[1], \"Δbest:\", best[1] - float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c22bd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loss: 14.882359343530393\n",
      "iter 0: flag=radius t=0.5 loss=1.2271700255679998\n",
      "iter 1: flag=radius t=0.5 loss=0.15885395032724095\n",
      "iter 2: flag=radius t=0.5 loss=0.11689316853411141\n",
      "iter 3: flag=radius t=0.5 loss=0.0790634517740953\n",
      "iter 4: flag=radius t=0.5 loss=0.049637269853174314\n",
      "iter 5: flag=radius t=0.5 loss=0.0369527069684916\n",
      "iter 6: flag=radius t=0.5 loss=0.030293802755239747\n",
      "iter 7: flag=radius t=0.5 loss=0.02767588239475441\n",
      "iter 8: flag=radius t=0.5 loss=0.0249693121558565\n",
      "iter 9: flag=radius t=0.5 loss=0.023130127942680948\n",
      "loss trend: [14.882359343530393, 1.2271700255679998, 0.15885395032724095, 0.11689316853411141, 0.0790634517740953, 0.049637269853174314, 0.0369527069684916, 0.030293802755239747, 0.02767588239475441, 0.0249693121558565, 0.023130127942680948]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "params0 = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss0 = float(closure())\n",
    "print(\"start loss:\", loss0)\n",
    "\n",
    "mu = test.alpha / test.optimizer.c\n",
    "history = [loss0]\n",
    "\n",
    "for k in range(10):  # few iterations to check trend\n",
    "    params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "    loss = closure()\n",
    "    q = params.clone().detach()\n",
    "    Gq = test.optimizer._Gradient(q, params, loss)\n",
    "    DG = test.optimizer._Hessian(q, params, loss)\n",
    "    DP = _compute_dprox(q, mu)\n",
    "    I_active = (torch.diagonal(DP) != 0)\n",
    "    kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "    sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "\n",
    "    dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "\n",
    "    # simple backtracking\n",
    "    ts = [1.0, 0.5, 0.25, 0.125, 0.0625]\n",
    "    chosen = None\n",
    "    best_loss = float('inf')\n",
    "    best_u = None\n",
    "    for t in ts:\n",
    "        q_t = q + t * dq\n",
    "        u_t = _compute_prox(q_t, mu)\n",
    "        backup = params.clone()\n",
    "        test.optimizer._update_parameters(u_t)\n",
    "        loss_t = float(closure())\n",
    "        test.optimizer._update_parameters(backup)\n",
    "        if loss_t < best_loss:\n",
    "            best_loss = loss_t\n",
    "            chosen = t\n",
    "            best_u = u_t.clone()\n",
    "\n",
    "    test.optimizer._update_parameters(best_u)\n",
    "    history.append(best_loss)\n",
    "    print(f\"iter {k}: flag={flag} t={chosen} loss={best_loss}\")\n",
    "\n",
    "print(\"loss trend:\", history)\n",
    "# restore original if you don't want to keep the change:\n",
    "# test.optimizer._update_parameters(params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "731380b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta=0.00 ||Gq||=4.149e-02 flag=radius best_loss=0.02125751619929503 Δ=-1.873e-03\n",
      "beta=0.25 ||Gq||=3.112e-02 flag=radius best_loss=0.028915530467504266 Δ=5.785e-03\n",
      "beta=0.50 ||Gq||=2.074e-02 flag=radius best_loss=0.052909690238746886 Δ=2.978e-02\n",
      "beta=0.75 ||Gq||=1.037e-02 flag=radius best_loss=0.09391576295462797 Δ=7.079e-02\n",
      "beta=1.00 ||Gq||=1.391e-16 flag=maxitr best_loss=0.15519202462791862 Δ=1.321e-01\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _ddphi, _compute_prox, _compute_dprox\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "grads = torch.autograd.grad(loss, test.optimizer.param_groups[0][\"params\"], create_graph=True, retain_graph=True)\n",
    "grad_flat = torch.cat([g.view(-1) for g in grads])\n",
    "D_nonconvex = torch.sign(params) * (_ddphi(torch.abs(params), test.th, test.gamma) - 1)\n",
    "rterm = test.alpha * D_nonconvex + grad_flat\n",
    "mu = test.alpha / test.optimizer.c\n",
    "\n",
    "for beta in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    q = params - (beta / test.optimizer.c) * rterm\n",
    "    Gq = test.optimizer._Gradient(q, params, loss)\n",
    "    DG = test.optimizer._Hessian(q, params, loss)\n",
    "    DP = _compute_dprox(q, mu)\n",
    "    I_active = (torch.diagonal(DP) != 0)\n",
    "    kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "    sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "    dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "\n",
    "    # evaluate best t along dq\n",
    "    ts = [1.0, 0.5, 0.25, 0.125]\n",
    "    best_loss = float('inf')\n",
    "    for t in ts:\n",
    "        q_t = q + t * dq\n",
    "        u_t = _compute_prox(q_t, mu)\n",
    "        backup = params.clone()\n",
    "        test.optimizer._update_parameters(u_t)\n",
    "        loss_t = float(closure())\n",
    "        test.optimizer._update_parameters(backup)\n",
    "        best_loss = min(best_loss, loss_t)\n",
    "\n",
    "    print(f\"beta={beta:.2f} ||Gq||={float(torch.norm(Gq)):.3e} flag={flag} best_loss={best_loss} Δ={best_loss - float(loss):.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bcb3d",
   "metadata": {},
   "source": [
    "## SSN(line-search) for the outer weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdp_model = model_outerweights(data, torch.relu, 2.0, regularization, 'SSN', loss_weights=(1.0, 0.0))\n",
    "model_result, weight, bias, output_weight = vdp_model.train(\n",
    "    inner_weights=weights, inner_bias=bias, outer_weights=outer_weights,\n",
    "    iterations=10000,\n",
    "    display_every=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb761ab",
   "metadata": {},
   "source": [
    "## SSN(trust-region) for the outer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdp_model = model_outerweights(data, torch.relu, 2.0, regularization, 'SSN_TR', loss_weights=(1.0, 0.0))\n",
    "model_result, weight, bias, output_weight = vdp_model.train(\n",
    "    inner_weights=weights, inner_bias=bias, outer_weights=outer_weights,\n",
    "    iterations=10000,\n",
    "    display_every=500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
