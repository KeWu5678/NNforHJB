{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the src directory to Python path so model.py can find ssn and net modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c2f23",
   "metadata": {},
   "source": [
    "Load the data that is generated from the open-loop optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ff901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 10:43:23.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mLoaded data with shape: (900,), dtype: [('x', '<f8', (2,)), ('dv', '<f8', (2,)), ('v', '<f8')]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "path = '../data_result/raw_data/VDP_beta_0.1_grid_30x30.npy'# Initialize the weights\n",
    "data = np.load(path)\n",
    "logger.info(f\"Loaded data with shape: {data.shape}, dtype: {data.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da1f3f",
   "metadata": {},
   "source": [
    "## SSN(trust-region) method for outer weights ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3914c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameter\n",
    "power = 2.1\n",
    "M = 50 # number greedy insertion selected\n",
    "num_iterations = 10\n",
    "loss_weights = (1.0, 0.0)\n",
    "pruning_threshold = 1e-15\n",
    "\n",
    "gamma = 5.0\n",
    "alpha = 1e-5\n",
    "lr_adam = 1e-5\n",
    "regularization = (gamma, alpha) \n",
    "th = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_configure_logger\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mModel initialized\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m150\u001b[0m - \u001b[1mTraining set: 810 samples, Validation set: 90 samples\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mData ranges - x: [-3.00, 3.00], v: [0.00, 10.96], dv: [-13.19, 13.13]\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mStarting network training session\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m227\u001b[0m - \u001b[1mUsing SSN_TR optimizer with alpha=1e-05, gamma=5.0, th=0.0\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m322\u001b[0m - \u001b[1mTraining hyperparameters: iterations=5000, batch_size=1620, display_every=1000\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mLoss weights: value=1.0, gradient=0.0\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 0: Train Loss = 28.301081, Val Loss = 27.105741\u001b[0m\n",
      "\u001b[32m2025-09-22 10:47:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 1000: Train Loss = 0.115093, Val Loss = 0.830573\u001b[0m\n",
      "\u001b[32m2025-09-22 10:48:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 2000: Train Loss = 0.115093, Val Loss = 0.830573\u001b[0m\n",
      "\u001b[32m2025-09-22 10:48:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 3000: Train Loss = 0.115093, Val Loss = 0.830573\u001b[0m\n",
      "\u001b[32m2025-09-22 10:48:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 4000: Train Loss = 0.115093, Val Loss = 0.830573\u001b[0m\n",
      "\u001b[32m2025-09-22 10:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mBest validation loss: 0.830573. Restored best model from /Users/ruizhechao/Documents/NNforHJB/train_history/model_best.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from src.model import model\n",
    "from src.greedy_insertion import _sample_uniform_sphere_points\n",
    "\n",
    "test = model(activation=torch.relu, power=power, regularization=regularization, optimizer='SSN_TR', loss_weights=loss_weights, th=th, train_outerweights=True)\n",
    "\n",
    "# prepare the data\n",
    "data_train, data_valid = test._prepare_data(data)\n",
    "\n",
    "W_hidden, b_hidden = _sample_uniform_sphere_points(M)\n",
    "test.train(data_train, data_valid, inner_weights=W_hidden, inner_bias=b_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594eef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 10:50:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_configure_logger\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mModel initialized\u001b[0m\n",
      "\u001b[32m2025-09-22 10:50:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m307\u001b[0m - \u001b[1mStarting network training session\u001b[0m\n",
      "\u001b[32m2025-09-22 10:50:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m227\u001b[0m - \u001b[1mUsing SSN_TR optimizer with alpha=1e-05, gamma=5.0, th=0.0\u001b[0m\n",
      "\u001b[32m2025-09-22 10:50:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m322\u001b[0m - \u001b[1mTraining hyperparameters: iterations=5000, batch_size=1620, display_every=1000\u001b[0m\n",
      "\u001b[32m2025-09-22 10:50:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m323\u001b[0m - \u001b[1mLoss weights: value=1.0, gradient=0.0\u001b[0m\n",
      "\u001b[32m2025-09-22 10:50:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 0: Train Loss = 16.611090, Val Loss = 59.024638\u001b[0m\n",
      "\u001b[32m2025-09-22 10:50:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 1000: Train Loss = 0.136831, Val Loss = 0.861861\u001b[0m\n",
      "\u001b[32m2025-09-22 10:51:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 2000: Train Loss = 0.136831, Val Loss = 0.861861\u001b[0m\n",
      "\u001b[32m2025-09-22 10:51:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 3000: Train Loss = 0.136831, Val Loss = 0.861861\u001b[0m\n",
      "\u001b[32m2025-09-22 10:51:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mEpoch 4000: Train Loss = 0.136831, Val Loss = 0.861861\u001b[0m\n",
      "\u001b[32m2025-09-22 10:51:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mBest validation loss: 0.861861. Restored best model from /Users/ruizhechao/Documents/NNforHJB/train_history/model_best.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_ls = model(activation=torch.relu, power=power, regularization=regularization, optimizer='SSN', loss_weights=loss_weights, th=th, train_outerweights=True)\n",
    "test.train(data_train, data_valid, inner_weights=W_hidden, inner_bias=b_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608838f",
   "metadata": {},
   "source": [
    "## Test Cases ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f27bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-22 10:43:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m227\u001b[0m - \u001b[1mUsing SSN_TR optimizer with alpha=1e-05, gamma=5.0, th=0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.ssn import SSN\n",
    "from src.ssn_tr import SSN_TR\n",
    "\n",
    "# Build net once if needed\n",
    "if test.net is None:\n",
    "    test._create_network(inner_weights=W_hidden, inner_bias=b_hidden)\n",
    "\n",
    "# Build optimizer once if needed\n",
    "if test.optimizer is None:\n",
    "    test._setup_optimizer()\n",
    "\n",
    "# Define closure for the current data tensors\n",
    "train_x_tensor, train_v_tensor, train_dv_tensor = data_train\n",
    "def closure():\n",
    "    if isinstance(test.optimizer, (SSN, SSN_TR)):\n",
    "        with torch.no_grad():\n",
    "            _, hidden_activations = test.net.forward_with_hidden(train_x_tensor.detach())\n",
    "        test.optimizer.hidden_activations = hidden_activations.detach()\n",
    "    total_loss, _, _ = test._compute_loss(train_x_tensor, train_v_tensor, train_dv_tensor)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f11963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Gq||: 136.8681762444426 ||lhs - rhs||: 0.0\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _ddphi\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "q = test.optimizer._transform_param2q(params, loss)\n",
    "Gq = test.optimizer._Gradient(q, params, loss)\n",
    "\n",
    "grads = torch.autograd.grad(loss, test.optimizer.param_groups[0][\"params\"], create_graph=True, retain_graph=True)\n",
    "grad_flat = torch.cat([g.view(-1) for g in grads])\n",
    "D_nonconvex = torch.sign(params) * (_ddphi(torch.abs(params), test.th, test.gamma) - 1)\n",
    "\n",
    "lhs = Gq\n",
    "rhs = test.optimizer.c * (q - params) + test.alpha * D_nonconvex + grad_flat\n",
    "print(\"||Gq||:\", float(torch.norm(lhs)), \"||lhs - rhs||:\", float(torch.norm(lhs - rhs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0183764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at t grid from params->prox(q):\n",
      "t=0.0 loss=38.21510561321624\n",
      "t=0.1 loss=38.21510492660367\n",
      "t=0.2 loss=38.21510607095796\n",
      "t=0.3 loss=38.21510561321624\n",
      "t=0.4 loss=38.21510652869967\n",
      "t=0.5 loss=38.21510561321624\n",
      "t=0.6 loss=38.21510561321624\n",
      "t=0.7 loss=38.215105613216245\n",
      "t=0.8 loss=38.215105613216245\n",
      "t=0.9 loss=38.215105613216245\n",
      "t=1.0 loss=38.215105613216245\n",
      "Δloss at t=1: tensor(7.1054e-15, dtype=torch.float64, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _compute_prox\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss0 = closure()\n",
    "\n",
    "mu = test.alpha / test.optimizer.c\n",
    "q = test.optimizer._transform_param2q(params, loss0)\n",
    "unew = _compute_prox(q, mu)\n",
    "\n",
    "vals = []\n",
    "ts = torch.linspace(0, 1, steps=11)\n",
    "for t in ts:\n",
    "    u_t = params*(1 - t) + unew*t\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    vals.append(float(closure()))\n",
    "    test.optimizer._update_parameters(backup)\n",
    "\n",
    "print(\"loss at t grid from params->prox(q):\")\n",
    "for t, v in zip(ts, vals):\n",
    "    print(f\"t={float(t):.1f} loss={v}\")\n",
    "print(\"Δloss at t=1:\", vals[-1] - loss0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2676e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpcg: flag radius pred -71.33386699443048 relres 0.1512846050380833 iters 1 ||dq|| 0.9999999999999999\n",
      "t=1.00000 loss=32.41331244223033 Δ=-5.80179317098591\n",
      "t=0.50000 loss=2.5480531923313716 Δ=-35.667052420884865\n",
      "t=0.25000 loss=12.189804984138405 Δ=-26.025300629077833\n",
      "t=0.12500 loss=23.15368325820984 Δ=-15.0614223550064\n",
      "t=0.06250 loss=30.17189165504805 Δ=-8.043213958168188\n",
      "t=0.03125 loss=34.06452212180356 Δ=-4.150583491412675\n",
      "best t: 0.5 best loss: 2.5480531923313716 Δbest: -35.667052420884865\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _compute_dprox, _compute_prox\n",
    "\n",
    "# Freeze current state\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "# Use q = params to avoid algebraic cancellation\n",
    "q = params.clone().detach()\n",
    "Gq = test.optimizer._Gradient(q, params, loss)          # now ≈ alpha*D_nonconvex + grad_flat\n",
    "DG = test.optimizer._Hessian(q, params, loss)\n",
    "mu = test.alpha / test.optimizer.c\n",
    "DP = _compute_dprox(q, mu)\n",
    "\n",
    "# One MPCG step\n",
    "from src.mpcg import mpcg\n",
    "I_active = (torch.diagonal(DP) != 0)\n",
    "kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "sigma = test.optimizer.sigma if isinstance(test.optimizer, (SSN_TR,)) else 0.0\n",
    "\n",
    "dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "print(\"mpcg: flag\", flag, \"pred\", pred, \"relres\", relres, \"iters\", iters, \"||dq||\", float(torch.norm(dq)))\n",
    "\n",
    "# Try a backtracking grid along dq: q_new = q + t*dq, u_new = prox(q_new)\n",
    "ts = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125]\n",
    "best = (None, float('inf'))\n",
    "for t in ts:\n",
    "    q_t = q + t * dq\n",
    "    u_t = _compute_prox(q_t, mu)\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    loss_t = float(closure())\n",
    "    test.optimizer._update_parameters(backup)\n",
    "    print(f\"t={t:.5f} loss={loss_t} Δ={loss_t - float(loss)}\")\n",
    "    if loss_t < best[1]:\n",
    "        best = (t, loss_t)\n",
    "print(\"best t:\", best[0], \"best loss:\", best[1], \"Δbest:\", best[1] - float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d31c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma: 1.0\n",
      "active (DP>0): 50\n"
     ]
    }
   ],
   "source": [
    "print(\"sigma:\", float(getattr(test.optimizer, \"sigma\", 0.0)))\n",
    "DP = __import__(\"src.utils\", fromlist=[\"_compute_dprox\"])._compute_dprox(\n",
    "    torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]]),\n",
    "    test.alpha / test.optimizer.c\n",
    ")\n",
    "print(\"active (DP>0):\", int((torch.diagonal(DP) > 0).sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb43883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpcg: flag radius pred -71.33386699443048 relres 0.1512846050380833 iters 1 ||dq|| 0.9999999999999999\n",
      "t=1.00000 loss=32.41331244223033 Δ=-5.80179317098591\n",
      "t=0.50000 loss=2.5480531923313716 Δ=-35.667052420884865\n",
      "t=0.25000 loss=12.189804984138405 Δ=-26.025300629077833\n",
      "t=0.12500 loss=23.15368325820984 Δ=-15.0614223550064\n",
      "t=0.06250 loss=30.17189165504805 Δ=-8.043213958168188\n",
      "t=0.03125 loss=34.06452212180356 Δ=-4.150583491412675\n",
      "best t: 0.5 best loss: 2.5480531923313716 Δbest: -35.667052420884865\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _compute_dprox, _compute_prox, _ddphi\n",
    "from src.mpcg import mpcg\n",
    "from src.ssn import SSN\n",
    "from src.ssn_tr import SSN_TR\n",
    "\n",
    "train_x_tensor, train_v_tensor, train_dv_tensor = data_train\n",
    "\n",
    "def closure():\n",
    "    if isinstance(test.optimizer, (SSN, SSN_TR)):\n",
    "        with torch.no_grad():\n",
    "            _, S = test.net.forward_with_hidden(train_x_tensor.detach())\n",
    "        test.optimizer.hidden_activations = S.detach()\n",
    "    total_loss, _, _ = test._compute_loss(train_x_tensor, train_v_tensor, train_dv_tensor)\n",
    "    return total_loss\n",
    "\n",
    "# Freeze state\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "# q = params\n",
    "q = params.clone().detach()\n",
    "Gq = test.optimizer._Gradient(q, params, loss)\n",
    "DG = test.optimizer._Hessian(q, params, loss)\n",
    "mu = test.alpha / test.optimizer.c\n",
    "DP = _compute_dprox(q, mu)\n",
    "\n",
    "I_active = (torch.diagonal(DP) != 0)\n",
    "kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "\n",
    "dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "print(\"mpcg: flag\", flag, \"pred\", pred, \"relres\", relres, \"iters\", iters, \"||dq||\", float(torch.norm(dq)))\n",
    "\n",
    "# Backtracking along dq\n",
    "ts = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125]\n",
    "best = (None, float('inf'))\n",
    "for t in ts:\n",
    "    q_t = q + t * dq\n",
    "    u_t = _compute_prox(q_t, mu)\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    loss_t = float(closure())\n",
    "    test.optimizer._update_parameters(backup)\n",
    "    print(f\"t={t:.5f} loss={loss_t} Δ={loss_t - float(loss)}\")\n",
    "    if loss_t < best[1]:\n",
    "        best = (t, loss_t)\n",
    "print(\"best t:\", best[0], \"best loss:\", best[1], \"Δbest:\", best[1] - float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c22bd456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loss: 38.21510561321624\n",
      "iter 0: flag=radius t=0.5 loss=2.5480531923313716\n",
      "iter 1: flag=radius t=0.5 loss=0.15064699374156462\n",
      "iter 2: flag=radius t=0.5 loss=0.10947923170336686\n",
      "iter 3: flag=radius t=0.5 loss=0.07109906685868055\n",
      "iter 4: flag=radius t=0.5 loss=0.05296490902586828\n",
      "iter 5: flag=radius t=0.5 loss=0.04083015756411104\n",
      "iter 6: flag=radius t=1.0 loss=0.03432765462993657\n",
      "iter 7: flag=radius t=0.5 loss=0.027993926795296657\n",
      "iter 8: flag=radius t=0.5 loss=0.025564907746785687\n",
      "iter 9: flag=radius t=0.5 loss=0.02356511665798449\n",
      "loss trend: [38.21510561321624, 2.5480531923313716, 0.15064699374156462, 0.10947923170336686, 0.07109906685868055, 0.05296490902586828, 0.04083015756411104, 0.03432765462993657, 0.027993926795296657, 0.025564907746785687, 0.02356511665798449]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "params0 = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss0 = float(closure())\n",
    "print(\"start loss:\", loss0)\n",
    "\n",
    "mu = test.alpha / test.optimizer.c\n",
    "history = [loss0]\n",
    "\n",
    "for k in range(10):  # few iterations to check trend\n",
    "    params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "    loss = closure()\n",
    "    q = params.clone().detach()\n",
    "    Gq = test.optimizer._Gradient(q, params, loss)\n",
    "    DG = test.optimizer._Hessian(q, params, loss)\n",
    "    DP = _compute_dprox(q, mu)\n",
    "    I_active = (torch.diagonal(DP) != 0)\n",
    "    kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "    sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "\n",
    "    dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "\n",
    "    # simple backtracking\n",
    "    ts = [1.0, 0.5, 0.25, 0.125, 0.0625]\n",
    "    chosen = None\n",
    "    best_loss = float('inf')\n",
    "    best_u = None\n",
    "    for t in ts:\n",
    "        q_t = q + t * dq\n",
    "        u_t = _compute_prox(q_t, mu)\n",
    "        backup = params.clone()\n",
    "        test.optimizer._update_parameters(u_t)\n",
    "        loss_t = float(closure())\n",
    "        test.optimizer._update_parameters(backup)\n",
    "        if loss_t < best_loss:\n",
    "            best_loss = loss_t\n",
    "            chosen = t\n",
    "            best_u = u_t.clone()\n",
    "\n",
    "    test.optimizer._update_parameters(best_u)\n",
    "    history.append(best_loss)\n",
    "    print(f\"iter {k}: flag={flag} t={chosen} loss={best_loss}\")\n",
    "\n",
    "print(\"loss trend:\", history)\n",
    "# restore original if you don't want to keep the change:\n",
    "# test.optimizer._update_parameters(params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731380b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta=0.00 ||Gq||=1.150e-02 flag=radius best_loss=0.02184191053258934 Δ=-1.723e-03\n",
      "beta=0.25 ||Gq||=8.627e-03 flag=radius best_loss=0.021798467732128454 Δ=-1.767e-03\n",
      "beta=0.50 ||Gq||=5.751e-03 flag=radius best_loss=0.02193777233292615 Δ=-1.627e-03\n",
      "beta=0.75 ||Gq||=2.876e-03 flag=radius best_loss=0.023128094518807398 Δ=-4.370e-04\n",
      "beta=1.00 ||Gq||=1.350e-16 flag=maxitr best_loss=0.028410341204405783 Δ=4.845e-03\n"
     ]
    }
   ],
   "source": [
    "from src.utils import _ddphi, _compute_prox, _compute_dprox\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "grads = torch.autograd.grad(loss, test.optimizer.param_groups[0][\"params\"], create_graph=True, retain_graph=True)\n",
    "grad_flat = torch.cat([g.view(-1) for g in grads])\n",
    "D_nonconvex = torch.sign(params) * (_ddphi(torch.abs(params), test.th, test.gamma) - 1)\n",
    "rterm = test.alpha * D_nonconvex + grad_flat\n",
    "mu = test.alpha / test.optimizer.c\n",
    "\n",
    "for beta in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    q = params - (beta / test.optimizer.c) * rterm\n",
    "    Gq = test.optimizer._Gradient(q, params, loss)\n",
    "    DG = test.optimizer._Hessian(q, params, loss)\n",
    "    DP = _compute_dprox(q, mu)\n",
    "    I_active = (torch.diagonal(DP) != 0)\n",
    "    kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "    sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "    dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "\n",
    "    # evaluate best t along dq\n",
    "    ts = [1.0, 0.5, 0.25, 0.125]\n",
    "    best_loss = float('inf')\n",
    "    for t in ts:\n",
    "        q_t = q + t * dq\n",
    "        u_t = _compute_prox(q_t, mu)\n",
    "        backup = params.clone()\n",
    "        test.optimizer._update_parameters(u_t)\n",
    "        loss_t = float(closure())\n",
    "        test.optimizer._update_parameters(backup)\n",
    "        best_loss = min(best_loss, loss_t)\n",
    "\n",
    "    print(f\"beta={beta:.2f} ||Gq||={float(torch.norm(Gq)):.3e} flag={flag} best_loss={best_loss} Δ={best_loss - float(loss):.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
