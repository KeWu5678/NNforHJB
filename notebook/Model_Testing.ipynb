{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7463c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ruizhechao/Documents/NNforHJB\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext nb_black\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, Path().absolute().parent.as_posix())\n",
    "from loguru import logger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c2f23",
   "metadata": {},
   "source": [
    "Load the data that is generated from the open-loop optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ff901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('x', '<f8', (2,)), ('dv', '<f8', (2,)), ('v', '<f8')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "import numpy as np\n",
    "\n",
    "path = 'rawdata/raw_data/VDP_beta_0.1_grid_30x30.npy'\n",
    "data = np.load(path)\n",
    "data.dtype\n",
    "# logger.info(f\"Loaded data with shape: {data.shape}, dtype: {data.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698d5f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.        , -3.        ],\n",
       "       [-2.79310345, -3.        ],\n",
       "       [-2.5862069 , -3.        ],\n",
       "       ...,\n",
       "       [ 2.5862069 ,  3.        ],\n",
       "       [ 2.79310345,  3.        ],\n",
       "       [ 3.        ,  3.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da1f3f",
   "metadata": {},
   "source": [
    "## SSN(trust-region) method for outer weights ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3914c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameter\n",
    "power = 2.1\n",
    "M = 50 # number greedy insertion selected\n",
    "num_iterations = 10\n",
    "loss_weights = (1.0, 0.0)\n",
    "pruning_threshold = 1e-15\n",
    "\n",
    "gamma = 5.0\n",
    "alpha = 1e-5\n",
    "lr_adam = 1e-5\n",
    "regularization = (gamma, alpha) \n",
    "th = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b289993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-13 17:11:03.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mModel initialized\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scr.model import model\n",
    "from scr.greedy_insertion import _sample_uniform_sphere_points\n",
    "import torch\n",
    "\n",
    "test = model(\n",
    "    alpha, \n",
    "    gamma,\n",
    "    optimizer='SSN',\n",
    "    activation=torch.relu, \n",
    "    train_outerweights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b4c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-13 17:11:04.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36m_prepare_data\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mTraining set: 810 samples, Validation set: 90 samples\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "# data is a structured numpy array with fields: 'x', 'dv', 'v'\n",
    "# convert to the dict format expected by model._prepare_data\n",
    "\n",
    "data_dict = {\n",
    "    \"x\": np.asarray(data[\"x\"], dtype=np.float64),    # shape (N, 2)\n",
    "    \"v\": np.asarray(data[\"v\"], dtype=np.float64),    # shape (N,)\n",
    "    \"dv\": np.asarray(data[\"dv\"], dtype=np.float64),  # shape (N, 2)\n",
    "}\n",
    "\n",
    "data_train, data_valid = test._prepare_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f015fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of scr.ssn failed: Traceback (most recent call last):\n",
      "  File \"/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/Users/ruizhechao/Documents/NNforHJB/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: step() requires a code object with 1 free vars, not 0\n",
      "]\n",
      "\u001b[32m2025-12-13 17:17:38.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36m_setup_optimizer\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mUsing SSN optimizer with alpha=1e-05, gamma=5.0, th=0.5\u001b[0m\n",
      "\u001b[32m2025-12-13 17:17:38.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m255\u001b[0m - \u001b[1mStarting network training session\u001b[0m\n",
      "\u001b[32m2025-12-13 17:17:38.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mEpoch 0: Train Loss = 28.434141, Val Loss = 37.703572\u001b[0m\n",
      "\u001b[32m2025-12-13 17:17:39.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mEpoch 100: Train Loss = 1.445665, Val Loss = 2.040714\u001b[0m\n",
      "\u001b[32m2025-12-13 17:17:40.227\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mEpoch 200: Train Loss = 1.347692, Val Loss = 2.106399\u001b[0m\n",
      "\u001b[32m2025-12-13 17:17:41.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mEpoch 300: Train Loss = 1.346596, Val Loss = 2.137413\u001b[0m\n",
      "\u001b[32m2025-12-13 17:17:42.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscr.model\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m302\u001b[0m - \u001b[1mBest validation loss: 1.969634 at iteration 121\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scr.model import model\n",
    "\n",
    "W_hidden, b_hidden = _sample_uniform_sphere_points(100)  # numpy arrays\n",
    "\n",
    "W_hidden_t = torch.as_tensor(W_hidden, dtype=torch.float64)\n",
    "b_hidden_t = torch.as_tensor(b_hidden, dtype=torch.float64).reshape(-1)  # bias must be (M,)\n",
    "\n",
    "test.train(\n",
    "    data_train, \n",
    "    data_valid, \n",
    "    inner_weights=W_hidden_t, \n",
    "    inner_bias=b_hidden_t,\n",
    "    iterations = 400,\n",
    "    display_every=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608838f",
   "metadata": {},
   "source": [
    "## Test Cases ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f27bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.ssn import SSN\n",
    "from src.ssn_tr import SSN_TR\n",
    "\n",
    "# Build net once if needed\n",
    "if test.net is None:\n",
    "    test._create_network(inner_weights=W_hidden, inner_bias=b_hidden)\n",
    "\n",
    "# Build optimizer once if needed\n",
    "if test.optimizer is None:\n",
    "    test._setup_optimizer()\n",
    "\n",
    "# Define closure for the current data tensors\n",
    "train_x_tensor, train_v_tensor, train_dv_tensor = data_train\n",
    "def closure():\n",
    "    if isinstance(test.optimizer, (SSN, SSN_TR)):\n",
    "        with torch.no_grad():\n",
    "            _, hidden_activations = test.net.forward_with_hidden(train_x_tensor.detach())\n",
    "        test.optimizer.hidden_activations = hidden_activations.detach()\n",
    "    total_loss, _, _ = test._compute_loss(train_x_tensor, train_v_tensor, train_dv_tensor)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f11963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import _ddphi\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "q = test.optimizer._transform_param2q(params, loss)\n",
    "Gq = test.optimizer._Gradient(q, params, loss)\n",
    "\n",
    "grads = torch.autograd.grad(loss, test.optimizer.param_groups[0][\"params\"], create_graph=True, retain_graph=True)\n",
    "grad_flat = torch.cat([g.view(-1) for g in grads])\n",
    "D_nonconvex = torch.sign(params) * (_ddphi(torch.abs(params), test.th, test.gamma) - 1)\n",
    "\n",
    "lhs = Gq\n",
    "rhs = test.optimizer.c * (q - params) + test.alpha * D_nonconvex + grad_flat\n",
    "print(\"||Gq||:\", float(torch.norm(lhs)), \"||lhs - rhs||:\", float(torch.norm(lhs - rhs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0183764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import _compute_prox\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss0 = closure()\n",
    "\n",
    "mu = test.alpha / test.optimizer.c\n",
    "q = test.optimizer._transform_param2q(params, loss0)\n",
    "unew = _compute_prox(q, mu)\n",
    "\n",
    "vals = []\n",
    "ts = torch.linspace(0, 1, steps=11)\n",
    "for t in ts:\n",
    "    u_t = params*(1 - t) + unew*t\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    vals.append(float(closure()))\n",
    "    test.optimizer._update_parameters(backup)\n",
    "\n",
    "print(\"loss at t grid from params->prox(q):\")\n",
    "for t, v in zip(ts, vals):\n",
    "    print(f\"t={float(t):.1f} loss={v}\")\n",
    "print(\"Δloss at t=1:\", vals[-1] - loss0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import _compute_dprox, _compute_prox\n",
    "\n",
    "# Freeze current state\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "# Use q = params to avoid algebraic cancellation\n",
    "q = params.clone().detach()\n",
    "Gq = test.optimizer._Gradient(q, params, loss)          # now ≈ alpha*D_nonconvex + grad_flat\n",
    "DG = test.optimizer._Hessian(q, params, loss)\n",
    "mu = test.alpha / test.optimizer.c\n",
    "DP = _compute_dprox(q, mu)\n",
    "\n",
    "# One MPCG step\n",
    "from src.mpcg import mpcg\n",
    "I_active = (torch.diagonal(DP) != 0)\n",
    "kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "sigma = test.optimizer.sigma if isinstance(test.optimizer, (SSN_TR,)) else 0.0\n",
    "\n",
    "dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "print(\"mpcg: flag\", flag, \"pred\", pred, \"relres\", relres, \"iters\", iters, \"||dq||\", float(torch.norm(dq)))\n",
    "\n",
    "# Try a backtracking grid along dq: q_new = q + t*dq, u_new = prox(q_new)\n",
    "ts = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125]\n",
    "best = (None, float('inf'))\n",
    "for t in ts:\n",
    "    q_t = q + t * dq\n",
    "    u_t = _compute_prox(q_t, mu)\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    loss_t = float(closure())\n",
    "    test.optimizer._update_parameters(backup)\n",
    "    print(f\"t={t:.5f} loss={loss_t} Δ={loss_t - float(loss)}\")\n",
    "    if loss_t < best[1]:\n",
    "        best = (t, loss_t)\n",
    "print(\"best t:\", best[0], \"best loss:\", best[1], \"Δbest:\", best[1] - float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sigma:\", float(getattr(test.optimizer, \"sigma\", 0.0)))\n",
    "DP = __import__(\"src.utils\", fromlist=[\"_compute_dprox\"])._compute_dprox(\n",
    "    torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]]),\n",
    "    test.alpha / test.optimizer.c\n",
    ")\n",
    "print(\"active (DP>0):\", int((torch.diagonal(DP) > 0).sum().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb43883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import _compute_dprox, _compute_prox, _ddphi\n",
    "from src.mpcg import mpcg\n",
    "from src.ssn import SSN\n",
    "from src.ssn_tr import SSN_TR\n",
    "\n",
    "train_x_tensor, train_v_tensor, train_dv_tensor = data_train\n",
    "\n",
    "def closure():\n",
    "    if isinstance(test.optimizer, (SSN, SSN_TR)):\n",
    "        with torch.no_grad():\n",
    "            _, S = test.net.forward_with_hidden(train_x_tensor.detach())\n",
    "        test.optimizer.hidden_activations = S.detach()\n",
    "    total_loss, _, _ = test._compute_loss(train_x_tensor, train_v_tensor, train_dv_tensor)\n",
    "    return total_loss\n",
    "\n",
    "# Freeze state\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "\n",
    "# q = params\n",
    "q = params.clone().detach()\n",
    "Gq = test.optimizer._Gradient(q, params, loss)\n",
    "DG = test.optimizer._Hessian(q, params, loss)\n",
    "mu = test.alpha / test.optimizer.c\n",
    "DP = _compute_dprox(q, mu)\n",
    "\n",
    "I_active = (torch.diagonal(DP) != 0)\n",
    "kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "\n",
    "dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "print(\"mpcg: flag\", flag, \"pred\", pred, \"relres\", relres, \"iters\", iters, \"||dq||\", float(torch.norm(dq)))\n",
    "\n",
    "# Backtracking along dq\n",
    "ts = [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125]\n",
    "best = (None, float('inf'))\n",
    "for t in ts:\n",
    "    q_t = q + t * dq\n",
    "    u_t = _compute_prox(q_t, mu)\n",
    "    backup = params.clone()\n",
    "    test.optimizer._update_parameters(u_t)\n",
    "    loss_t = float(closure())\n",
    "    test.optimizer._update_parameters(backup)\n",
    "    print(f\"t={t:.5f} loss={loss_t} Δ={loss_t - float(loss)}\")\n",
    "    if loss_t < best[1]:\n",
    "        best = (t, loss_t)\n",
    "print(\"best t:\", best[0], \"best loss:\", best[1], \"Δbest:\", best[1] - float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22bd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "params0 = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss0 = float(closure())\n",
    "print(\"start loss:\", loss0)\n",
    "\n",
    "mu = test.alpha / test.optimizer.c\n",
    "history = [loss0]\n",
    "\n",
    "for k in range(10):  # few iterations to check trend\n",
    "    params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "    loss = closure()\n",
    "    q = params.clone().detach()\n",
    "    Gq = test.optimizer._Gradient(q, params, loss)\n",
    "    DG = test.optimizer._Hessian(q, params, loss)\n",
    "    DP = _compute_dprox(q, mu)\n",
    "    I_active = (torch.diagonal(DP) != 0)\n",
    "    kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "    sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "\n",
    "    dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "\n",
    "    # simple backtracking\n",
    "    ts = [1.0, 0.5, 0.25, 0.125, 0.0625]\n",
    "    chosen = None\n",
    "    best_loss = float('inf')\n",
    "    best_u = None\n",
    "    for t in ts:\n",
    "        q_t = q + t * dq\n",
    "        u_t = _compute_prox(q_t, mu)\n",
    "        backup = params.clone()\n",
    "        test.optimizer._update_parameters(u_t)\n",
    "        loss_t = float(closure())\n",
    "        test.optimizer._update_parameters(backup)\n",
    "        if loss_t < best_loss:\n",
    "            best_loss = loss_t\n",
    "            chosen = t\n",
    "            best_u = u_t.clone()\n",
    "\n",
    "    test.optimizer._update_parameters(best_u)\n",
    "    history.append(best_loss)\n",
    "    print(f\"iter {k}: flag={flag} t={chosen} loss={best_loss}\")\n",
    "\n",
    "print(\"loss trend:\", history)\n",
    "# restore original if you don't want to keep the change:\n",
    "# test.optimizer._update_parameters(params0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731380b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import _ddphi, _compute_prox, _compute_dprox\n",
    "\n",
    "params = torch.cat([p.view(-1) for p in test.optimizer.param_groups[0][\"params\"]])\n",
    "loss = closure()\n",
    "grads = torch.autograd.grad(loss, test.optimizer.param_groups[0][\"params\"], create_graph=True, retain_graph=True)\n",
    "grad_flat = torch.cat([g.view(-1) for g in grads])\n",
    "D_nonconvex = torch.sign(params) * (_ddphi(torch.abs(params), test.th, test.gamma) - 1)\n",
    "rterm = test.alpha * D_nonconvex + grad_flat\n",
    "mu = test.alpha / test.optimizer.c\n",
    "\n",
    "for beta in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    q = params - (beta / test.optimizer.c) * rterm\n",
    "    Gq = test.optimizer._Gradient(q, params, loss)\n",
    "    DG = test.optimizer._Hessian(q, params, loss)\n",
    "    DP = _compute_dprox(q, mu)\n",
    "    I_active = (torch.diagonal(DP) != 0)\n",
    "    kmaxit = max(1, int(2 * I_active.sum().item()))\n",
    "    sigma = getattr(test.optimizer, \"sigma\", 0.0)\n",
    "    dq, flag, pred, relres, iters = mpcg(DG, -Gq, 1e-3, kmaxit, sigma, DP)\n",
    "\n",
    "    # evaluate best t along dq\n",
    "    ts = [1.0, 0.5, 0.25, 0.125]\n",
    "    best_loss = float('inf')\n",
    "    for t in ts:\n",
    "        q_t = q + t * dq\n",
    "        u_t = _compute_prox(q_t, mu)\n",
    "        backup = params.clone()\n",
    "        test.optimizer._update_parameters(u_t)\n",
    "        loss_t = float(closure())\n",
    "        test.optimizer._update_parameters(backup)\n",
    "        best_loss = min(best_loss, loss_t)\n",
    "\n",
    "    print(f\"beta={beta:.2f} ||Gq||={float(torch.norm(Gq)):.3e} flag={flag} best_loss={best_loss} Δ={best_loss - float(loss):.3e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
